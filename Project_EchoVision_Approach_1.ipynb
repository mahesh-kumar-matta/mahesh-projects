{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ac7ed5-4ab2-4053-bb0d-531ba3b043fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b109d2c6f652426ab5a740e5dafda4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDetrForObjectDetection LOAD REPORT\u001b[0m from: facebook/detr-resnet-50\n",
      "Key                                                            | Status     |  | \n",
      "---------------------------------------------------------------+------------+--+-\n",
      "model.backbone.model.layer2.0.downsample.1.num_batches_tracked | UNEXPECTED |  | \n",
      "model.backbone.model.layer1.0.downsample.1.num_batches_tracked | UNEXPECTED |  | \n",
      "model.backbone.model.layer4.0.downsample.1.num_batches_tracked | UNEXPECTED |  | \n",
      "model.backbone.model.layer3.0.downsample.1.num_batches_tracked | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d7d6b65c524920a948d95866002ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.1.weight to fine_acoustics.lm_heads.0.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.2.weight to fine_acoustics.lm_heads.1.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.3.weight to fine_acoustics.lm_heads.2.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.4.weight to fine_acoustics.lm_heads.3.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.5.weight to fine_acoustics.lm_heads.4.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.6.weight to fine_acoustics.lm_heads.5.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.7.weight to fine_acoustics.lm_heads.6.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "Both `max_new_tokens` (=768) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=28) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "from scipy.io.wavfile import write\n",
    "from IPython.display import Audio as IPythonAudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load image\n",
    "image = Image.open(\"images/image_1.jpeg\")\n",
    "\n",
    "# Step-1 : object detection\n",
    "object_detection = pipeline(\n",
    "    task=\"object-detection\",\n",
    "    model=\"facebook/detr-resnet-50\",\n",
    ")\n",
    "\n",
    "detected_objects = object_detection(image)\n",
    "\n",
    "# Step-2: call custom function to extract Labels from above json object and Convert to Text\n",
    "# function to extract Labels from above object json and Convert to Text\n",
    "def detr_pipeline_to_text(detections, conf_threshold=0.7):\n",
    "    detected = []\n",
    "\n",
    "    for obj in detections:\n",
    "        if obj[\"score\"] >= conf_threshold:\n",
    "            detected.append(obj[\"label\"])\n",
    "\n",
    "    if not detected:\n",
    "        return \"No confident objects were detected in the image.\"\n",
    "\n",
    "    counts = Counter(detected)\n",
    "\n",
    "    parts = []\n",
    "    for obj, count in counts.items():\n",
    "        if count == 1:\n",
    "            parts.append(f\"a {obj}\")\n",
    "        else:\n",
    "            parts.append(f\"{count} {obj}s\")\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        return f\"The image contains {parts[0]}.\"\n",
    "    else:\n",
    "        return \"The image contains \" + \", \".join(parts[:-1]) + \" and \" + parts[-1] + \".\"\n",
    "        \n",
    "result_text = detr_pipeline_to_text(detected_objects, conf_threshold=0.7)\n",
    "\n",
    "# Step-3: \n",
    "# create object for text-to-speech\n",
    "narrator = pipeline(\n",
    "    task=\"text-to-speech\",\n",
    "    model=\"suno/bark-small\",\n",
    ")\n",
    "\n",
    "# convert from text to speech\n",
    "narrated_text = narrator(result_text)\n",
    "\n",
    "# Play the audio\n",
    "IPythonAudio(\n",
    "    narrated_text[\"audio\"],\n",
    "    rate=narrated_text[\"sampling_rate\"]\n",
    ")\n",
    "\n",
    "# to save audio file on system\n",
    "audio = narrated_text[\"audio\"]\n",
    "sr = narrated_text[\"sampling_rate\"]\n",
    "# Ensure float32 format\n",
    "audio = np.asarray(audio, dtype=np.float32) \n",
    "write(\"output1.wav\", sr, audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85ca137-2c4a-4849-8313-9aa1b1b1e76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3252"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del object_detection\n",
    "del narrator\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73f1fe-b91f-4172-bd7e-d9be2f095780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
