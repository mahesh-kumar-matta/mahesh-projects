{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a64d3-9bc7-4867-8b2e-6c1d6c370c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain\n",
    "#! pip install sentence-transformers\n",
    "#! pip install langchain-huggingface\n",
    "#! pip install langchain-chroma\n",
    "# !pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9a5105-8cf4-46e5-be4e-0486b68731f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588bd2925ea0421b8db190059925f1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mMPNetModel LOAD REPORT\u001b[0m from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "C:\\Users\\mmatta\\AppData\\Local\\Temp\\ipykernel_17504\\2211975096.py:42: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  db = Chroma(collection_name=\"vector_database\",\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"German psychiatrist and pathologist Alois Alzheimer first described Alzheimer's disease in 1906.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ClinRAG: Clinical Knowledge Assistant RAG-Based Medical Question Answering System\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: Load Documents\n",
    "# ==============================\n",
    "loader = DirectoryLoader('wiki_documents', glob=\"*.txt\", loader_cls=TextLoader)\n",
    "kb_docs = loader.load()\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: Chunking\n",
    "# ==============================\n",
    "\n",
    "# Chunk the loaded documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(kb_docs)\n",
    "\n",
    "# ==============================\n",
    "# STEP 3: Embedding Model\n",
    "# ==============================\n",
    "# Create object for embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# STEP 4: Vector Database\n",
    "# ==============================\n",
    "# Initialize the database connection\n",
    "# If database exist, it will connect with the collection_name and persist_directory\n",
    "# Otherwise a new collection will be created\n",
    "db = Chroma(collection_name=\"vector_database\", \n",
    "            embedding_function=embedding_model, \n",
    "            persist_directory=\"./chroma_db_\")\n",
    "\n",
    "# Insert chunks in the vector database\n",
    "# Only add documents if DB is empty\n",
    "if db._collection.count() == 0:\n",
    "    db.add_documents(documents=chunks)\n",
    "\n",
    "# ==============================\n",
    "# STEP 5 & 6: Retriever\n",
    "# ==============================\n",
    "# Create Retriever. \n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# ==============================\n",
    "# STEP 7: Prompt Template\n",
    "# ==============================\n",
    "# Define Prompt Template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "Answer the question based on the above context: {question}.\n",
    "Provide a detailed answer.\n",
    "Don’t justify your answers.\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
    "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        PROMPT_TEMPLATE\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Chat Model/LLM Configuration\n",
    "# ==============================\n",
    "#load Key\n",
    "f = open(\"keys/.google_api_key.txt\")\n",
    "GOOGLE_API_KEY = f.read()\n",
    "\n",
    "# Initialize LLM\n",
    "chat_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "#Initialize a Output Parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Format Retrieved Documents. Helper function.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# ==============================\n",
    "# RAG Chain\n",
    "# ==============================\n",
    "# Create Generator Chain\n",
    "generator_chain = prompt_template | chat_model | parser\n",
    "\n",
    "# Define a RAG Chain\n",
    "rag_chain = {\n",
    "    \"context\": retriever | format_docs, \n",
    "    \"question\": RunnablePassthrough()\n",
    "} | generator_chain\n",
    "\n",
    "# Invoke the Chain\n",
    "query = 'Who first described Alzheimer’s disease?'\n",
    "\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d31aa-f227-4b74-9b37-7cf3d0e14c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
